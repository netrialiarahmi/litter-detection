{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download Data","metadata":{}},{"cell_type":"code","source":"!wget https://www.seanoe.org/data/00858/96963/data/106157.tar.gz\n!tar -xzvf 106157.tar.gz > /dev/null 2>&1\n!rm 106157.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:14:37.295683Z","iopub.execute_input":"2024-11-19T13:14:37.296240Z","iopub.status.idle":"2024-11-19T13:46:07.137210Z","shell.execute_reply.started":"2024-11-19T13:14:37.296195Z","shell.execute_reply":"2024-11-19T13:46:07.136106Z"}},"outputs":[{"name":"stdout","text":"--2024-11-19 13:14:38--  https://www.seanoe.org/data/00858/96963/data/106157.tar.gz\nResolving www.seanoe.org (www.seanoe.org)... 134.246.142.39\nConnecting to www.seanoe.org (www.seanoe.org)|134.246.142.39|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9489309612 (8.8G) [application/x-gzip]\nSaving to: '106157.tar.gz'\n\n106157.tar.gz       100%[===================>]   8.84G  3.74MB/s    in 29m 46s \n\n2024-11-19 13:44:24 (5.07 MB/s) - '106157.tar.gz' saved [9489309612/9489309612]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Library","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision import transforms\nimport torch.optim as optim\nfrom torchmetrics import MeanMetric\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torch.nn.functional as F\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:07.139221Z","iopub.execute_input":"2024-11-19T13:46:07.139533Z","iopub.status.idle":"2024-11-19T13:46:14.830890Z","shell.execute_reply.started":"2024-11-19T13:46:07.139502Z","shell.execute_reply":"2024-11-19T13:46:14.830165Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"IMAGE_SIZE = (224, 224)\nNUM_CLASSES = 13\nBOX_PRED_INCELL = 4\nEPOCHS = 1\nBATCH_SIZE = 16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:14.831869Z","iopub.execute_input":"2024-11-19T13:46:14.832290Z","iopub.status.idle":"2024-11-19T13:46:14.893673Z","shell.execute_reply.started":"2024-11-19T13:46:14.832261Z","shell.execute_reply":"2024-11-19T13:46:14.892473Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# import json\n\n# # Fungsi untuk memuat anotasi dari file JSON\n# def load_annotations(json_file):\n#     with open(json_file) as f:\n#         data = json.load(f)\n#     return data\n\n# # Fungsi untuk mendapatkan jumlah kelas dan kelas unik\n# def get_class_info(annotations_json):\n#     annotations = load_annotations(annotations_json)\n#     categories = annotations.get('categories', [])\n    \n#     # Menghitung jumlah kelas unik\n#     num_classes = len(categories)\n    \n#     # Membuat daftar kelas unik (ID dan nama)\n#     class_names = {category['id']: category['name'] for category in categories}\n    \n#     return num_classes, class_names\n\n# # Contoh penggunaan\n# annotations_path = '/kaggle/working/BePLi_dataset_v2/plastic_coco/annotation/train.json'  # Ganti dengan path ke file anotasi JSON\n# num_classes, class_names = get_class_info(annotations_path)\n\n# print(f\"Jumlah kelas dalam dataset: {num_classes}\")\n# print(\"Kelas unik dalam dataset:\")\n# for class_id, class_name in class_names.items():\n#     print(f\"ID: {class_id}, Nama: {class_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:14.895583Z","iopub.execute_input":"2024-11-19T13:46:14.895869Z","iopub.status.idle":"2024-11-19T13:46:14.913515Z","shell.execute_reply.started":"2024-11-19T13:46:14.895844Z","shell.execute_reply":"2024-11-19T13:46:14.912756Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Pipline Data","metadata":{}},{"cell_type":"code","source":"ANNOTATION_FOLDER = '/kaggle/working/BePLi_dataset_v2/plastic_coco/annotation'\nIMAGE_FOLDER = '/kaggle/working/BePLi_dataset_v2/plastic_coco/images/original_images'\n\n# Fungsi untuk memuat anotasi dari file JSON\ndef load_annotations(json_file):\n    with open(json_file) as f:\n        data = f.read()\n    return json.loads(data)\n\n# Fungsi untuk padding data\ndef pad_data(data, max_length, padding_value):\n    padded_data = []\n    for item_list in data:\n        padded_data.append(item_list + [padding_value] * (max_length - len(item_list)))\n    return padded_data\n\n# Fungsi untuk memuat data gambar, bbox, dan kategori dari anotasi\ndef load_data(annotations, split):\n    images = []\n    all_bboxes = []\n    all_categories = []\n    image_ids = set()\n    max_bboxes = 0\n    target_size = IMAGE_SIZE[0]\n\n    for annotation in annotations['annotations']:\n        image_id = annotation['image_id']\n        bbox = annotation['bbox']\n        category_id = annotation['category_id']\n\n        if image_id not in image_ids:\n            image_info = next((img for img in annotations['images'] if img['id'] == image_id), None)\n            if image_info:\n                scale_x = target_size / image_info['width']\n                scale_y = target_size / image_info['height']\n                image_path = os.path.join(IMAGE_FOLDER, image_info['file_name'])\n                images.append(image_path)\n                all_bboxes.append([])\n                all_categories.append([])\n                image_ids.add(image_id)\n\n        img_index = images.index(os.path.join(IMAGE_FOLDER, image_info['file_name']))\n        scaled_bbox = [\n            bbox[0] * scale_x,\n            bbox[1] * scale_y,\n            bbox[2] * scale_x,\n            bbox[3] * scale_y\n        ]\n\n        all_bboxes[img_index].append(scaled_bbox)\n        all_categories[img_index].append(category_id-1)\n        max_bboxes = max(max_bboxes, len(all_bboxes[img_index]))\n\n    all_bboxes = pad_data(all_bboxes, max_bboxes, [0, 0, 0, 0])\n    all_categories = pad_data(all_categories, max_bboxes, 0)\n    return images, all_bboxes, all_categories\n\n# Fungsi untuk preprocess gambar\ndef preprocess_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    transform = transforms.Compose([\n        transforms.Resize(IMAGE_SIZE),\n        transforms.ToTensor()\n    ])\n    return transform(image)\n\n# Custom Dataset untuk PyTorch\nclass PlasticDataset(Dataset):\n    def __init__(self, annotations, split):\n        self.images, self.bboxes, self.categories = load_data(annotations, split)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n        image = preprocess_image(image_path)\n        bboxes = torch.tensor(self.bboxes[idx], dtype=torch.float32)\n        categories = torch.tensor(self.categories[idx], dtype=torch.int64)\n        return image, bboxes, categories\n\n# Fungsi untuk membuat DataLoader\ndef create_dataloader(annotations_json, split, batch_size=BATCH_SIZE, shuffle=True):\n    annotations = load_annotations(annotations_json)\n    dataset = PlasticDataset(annotations, split)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n    return dataloader\n\n# Membuat DataLoader untuk train, val, dan test\ntrain_loader = create_dataloader(os.path.join(ANNOTATION_FOLDER, 'train.json'), 'train')\nval_loader = create_dataloader(os.path.join(ANNOTATION_FOLDER, 'val.json'), 'val')\ntest_loader = create_dataloader(os.path.join(ANNOTATION_FOLDER, 'test.json'), 'test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:14.914975Z","iopub.execute_input":"2024-11-19T13:46:14.915521Z","iopub.status.idle":"2024-11-19T13:46:18.906121Z","shell.execute_reply.started":"2024-11-19T13:46:14.915468Z","shell.execute_reply":"2024-11-19T13:46:18.905176Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # Membuat data dummy sesuai ukuran input (672x672)\n# dummy_input = torch.randn(2, 3, 224, 224)  # Batch size 1, 3 channel (RGB), 672x672 image\n\n# # Membuat model backbone\n# backbone_model = Backbone(image_size=IMAGE_SIZE)\n\n# # Jalankan model dengan data dummy\n# output = backbone_model(dummy_input)\n\n# # Cetak output untuk memeriksa apakah hook menangkap layer intermediate\n# for i, out in enumerate(output):\n#     print(f\"Output dari layer {i+1} - shape: {out.shape}\")\n# print(output[1].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:18.907901Z","iopub.execute_input":"2024-11-19T13:46:18.908307Z","iopub.status.idle":"2024-11-19T13:46:18.912327Z","shell.execute_reply.started":"2024-11-19T13:46:18.908265Z","shell.execute_reply":"2024-11-19T13:46:18.911469Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# MultiScale EfficientNet Backbone","metadata":{}},{"cell_type":"markdown","source":"![](https://images-provider.frontiersin.org/api/ipx/w=1200&f=png/https://www.frontiersin.org/files/Articles/881021/fnbot-16-881021-HTML/image_m/fnbot-16-881021-g001.jpg)","metadata":{}},{"cell_type":"code","source":"def Backbone(image_size=IMAGE_SIZE):\n    base_model = models.efficientnet_b0(pretrained=True)\n    for param in base_model.parameters():\n        param.requires_grad = False\n    intermediate_layers = ['features.3.1.block.2', 'features.5.1.block.2', 'features.8.1.block.2']\n    intermediate_outputs = []\n    hooks = []\n\n    def register_hooks(module, idx):\n        def hook(module, input, output):\n            intermediate_outputs.append(output)\n        return module.register_forward_hook(hook)\n\n    for idx, (name, layer) in enumerate(base_model.named_modules()):\n        if name in intermediate_layers:\n            hooks.append(register_hooks(layer, idx))\n\n    class BackboneModel(nn.Module):\n        def __init__(self, base_model):\n            super(BackboneModel, self).__init__()\n            self.base_model = base_model\n\n        def forward(self, x):\n            nonlocal intermediate_outputs\n            intermediate_outputs = []\n            x = self.base_model.features(x)\n            outputs = intermediate_outputs + [x]\n            return outputs\n\n    backbone_model = BackboneModel(base_model)\n    return backbone_model\n\nclass Head(nn.Module):\n    def __init__(self, num_classes=13, num_bbox_pred_incell=2, ks=(1, 1), chnl=1280):\n        super(Head, self).__init__()\n        self.num_classes = num_classes\n        self.num_bbox_pred_incell = num_bbox_pred_incell\n        self.bbox = nn.Conv2d(chnl, 4 * num_bbox_pred_incell, kernel_size=ks, stride=ks)\n        self.probs = nn.Conv2d(chnl, num_classes * num_bbox_pred_incell, kernel_size=ks, stride=ks)\n        self.sigmoid = nn.Sigmoid()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        boxes = self.bbox(x).reshape(x.size(0), -1, 4)\n        cls = self.sigmoid(self.probs(x)).reshape(x.size(0), -1, self.num_classes)\n        conf = self.softmax(self.probs(x)).reshape(x.size(0), -1, self.num_classes)\n        return boxes, cls, conf\n\nclass MultiScaleHead(nn.Module):\n    def __init__(self, num_classes=13, num_bbox_pred_incell=4):\n        super(MultiScaleHead, self).__init__()\n        self.head1 = Head(num_classes=num_classes, num_bbox_pred_incell=num_bbox_pred_incell, ks=(4, 4), chnl=240)\n        self.head2 = Head(num_classes=num_classes, num_bbox_pred_incell=num_bbox_pred_incell, ks=(2, 2), chnl=672)\n        self.head3 = Head(num_classes=num_classes, num_bbox_pred_incell=num_bbox_pred_incell, ks=(1, 1), chnl=1280)\n\n    def forward(self, inputs):\n        feature_map_3 = inputs[0]\n        feature_map_4 = inputs[1]\n        feature_map_5 = inputs[2]\n        boxes_3, cls_3, conf_3 = self.head1(feature_map_3)\n        boxes_4, cls_4, conf_4 = self.head2(feature_map_4)\n        boxes_5, cls_5, conf_5 = self.head3(feature_map_5)\n\n        boxes = torch.cat([boxes_3, boxes_4, boxes_5], dim=1)\n        cls = torch.cat([cls_3, cls_4, cls_5], dim=1)\n        conf = torch.cat([conf_3, conf_4, conf_5], dim=1)\n\n        return boxes, cls, cls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:18.913389Z","iopub.execute_input":"2024-11-19T13:46:18.913634Z","iopub.status.idle":"2024-11-19T13:46:18.931970Z","shell.execute_reply.started":"2024-11-19T13:46:18.913609Z","shell.execute_reply":"2024-11-19T13:46:18.931048Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Select Predict Box with IoU","metadata":{}},{"cell_type":"markdown","source":"$IoU(\\hat{\\beta}, \\beta)= \\frac{ \\hat{\\beta} \\, \\cap \\, \\beta}{\\hat{\\beta} \\, \\cup \\, \\beta}$ \\\r\n![](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2016/09/iou_equation.png)","metadata":{}},{"cell_type":"code","source":"def calculate_iou(box1, box2):\n    box1_x_min = box1[..., 0] - box1[..., 2] / 2\n    box1_y_min = box1[..., 1] - box1[..., 3] / 2\n    box1_x_max = box1[..., 0] + box1[..., 2] / 2\n    box1_y_max = box1[..., 1] + box1[..., 3] / 2\n\n    box2_x_min = box2[..., 0]\n    box2_y_min = box2[..., 1]\n    box2_x_max = box2[..., 0] + box2[..., 2]\n    box2_y_max = box2[..., 1] + box2[..., 3]\n\n    x1 = torch.maximum(box1_x_min, box2_x_min)\n    y1 = torch.maximum(box1_y_min, box2_y_min)\n    x2 = torch.minimum(box1_x_max, box2_x_max)\n    y2 = torch.minimum(box1_y_max, box2_y_max)\n\n    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n    area_box1 = (box1_x_max - box1_x_min) * (box1_y_max - box1_y_min)\n    area_box2 = (box2_x_max - box2_x_min) * (box2_y_max - box2_y_min)\n    union = area_box1 + area_box2 - intersection\n\n    return torch.where(union > 0, intersection / union, torch.zeros_like(intersection)).to(device)\n\nclass SelectedPreds(nn.Module):\n    def __init__(self):\n        super(SelectedPreds, self).__init__()\n\n    def forward(self, pred_bboxes, gt_bboxes, pred_classes, pred_confidences):\n        batch_size = pred_bboxes.size(0)\n        num_ground_truth = gt_bboxes.size(1)\n\n        pred_bboxes_exp = pred_bboxes.unsqueeze(2)\n        gt_bboxes_exp = gt_bboxes.unsqueeze(1)\n        iou_matrix = calculate_iou(pred_bboxes_exp, gt_bboxes_exp)\n        iou_sorted_indices = torch.argsort(iou_matrix, dim=1, descending=True)\n\n        matched_pred_bboxes = []\n        matched_pred_classes = []\n        matched_pred_confidences = []\n\n        for b in range(batch_size):\n            selected_bboxes = []\n            selected_classes = []\n            selected_confidences = []\n            used_pred_indices = set()\n\n            for gt_idx in range(num_ground_truth):\n                candidates = iou_sorted_indices[b, :, gt_idx]\n                available_candidates = [c for c in candidates if c not in used_pred_indices]\n\n                if available_candidates:\n                    selected_candidate = available_candidates[0]\n                    selected_bboxes.append(pred_bboxes[b, selected_candidate].unsqueeze(0))\n                    selected_classes.append(pred_classes[b, selected_candidate].unsqueeze(0))\n                    selected_confidences.append(pred_confidences[b, selected_candidate].unsqueeze(0))\n                    used_pred_indices.add(selected_candidate)\n                else:\n                    selected_bboxes.append(torch.zeros(1, 4).to(device))\n                    selected_classes.append(torch.zeros(1, pred_classes.size(-1)).to(device))\n                    selected_confidences.append(torch.zeros(1, 1).to(device))\n\n            matched_pred_bboxes.append(torch.cat(selected_bboxes, dim=0))\n            matched_pred_classes.append(torch.cat(selected_classes, dim=0))\n            matched_pred_confidences.append(torch.cat(selected_confidences, dim=0))\n\n        matched_pred_bboxes = torch.stack(matched_pred_bboxes).to(device)\n        matched_pred_classes = torch.stack(matched_pred_classes).to(device)\n        matched_pred_confidences = torch.stack(matched_pred_confidences).to(device)\n\n        return matched_pred_bboxes, matched_pred_classes, matched_pred_confidences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:18.933391Z","iopub.execute_input":"2024-11-19T13:46:18.933735Z","iopub.status.idle":"2024-11-19T13:46:18.951437Z","shell.execute_reply.started":"2024-11-19T13:46:18.933700Z","shell.execute_reply":"2024-11-19T13:46:18.950480Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Create Model and Loss Function YOLOv8","metadata":{}},{"cell_type":"markdown","source":"\\begin{align}\r\n\\mathcal{L} = & \\; \\frac{\\lambda_{\\text{box}}}{N_{\\text{pos}}} \\sum_{x, y} 1_{c_{x, y}^*} \\left[ (1 - q_{x, y}) + \\frac{\\| b_{x, y} - \\hat{b}_{x, y} \\|_2^2}{\\rho^2} + \\alpha_{x, y} \\nu_{x, y} \\right] \\\\\r\n& + \\frac{\\lambda_{\\text{cls}}}{N_{\\text{pos}}} \\sum_{x, y} \\sum_{c \\in \\text{classes}} y_c \\log(\\hat{y}_c) + (1 - y_c) \\log(1 - \\hat{y}_c) \\\\\r\n& + \\frac{\\lambda_{\\text{dfl}}}{N_{\\text{pos}}} \\sum_{x, y} 1_{c_{x, y}^*} \\left[ - \\left( q(x, y) + 1 - q_{x, y} \\right) \\log(\\hat{q}_{x, y}) + (q_{x, y} - q(x, y) - 1) \\log(1 - \\hat{q}_{x, y}) \\right]\r\n\\end{align}\r\n\r\n\\begin{align}\r\nq_{x,y}&=IoU(\\hat{\\beta}_{x,y}, \\beta_{x,y})=\\frac{\\hat{\\beta}_{x,y} \\cap \\beta_{x,y}}{\\hat{\\beta}_{x,y} \\cup \\beta_{x,y}} \\\\\r\nv_{x,y}&=\\frac{4}{\\pi^2}(arctan(\\frac{w_{x,y}}{h_{x,y}})-arctan(\\frac{\\hat{w}_{x,y}}{\\hat{h}_{x,y}}))^2 \\\\\r\n\\alpha_{x,y} &=\\frac{v}{1-q_{x,y}} \\\\\r\n\\end{align}","metadata":{}},{"cell_type":"code","source":"class ObjectDetectionModel(nn.Module):\n    def __init__(self, backbone, head, lbox=5.5, lcls=1.0, ldfl=2.5):\n        super(ObjectDetectionModel, self).__init__()\n        self.backbone = backbone\n        self.head = head\n        self.select_preds = SelectedPreds()\n        self.lbox = lbox\n        self.lcls = lcls\n        self.ldfl = ldfl\n\n    def calculate_box_loss(self, box_true, box_preds, N_pos):\n        iou = calculate_iou(box_preds, box_true)\n        box_preds_xmin = box_preds[..., 0] - box_preds[..., 2] / 2\n        box_preds_ymin = box_preds[..., 1] - box_preds[..., 3] / 2\n        box_preds_xmax = box_preds[..., 0] + box_preds[..., 2] / 2\n        box_preds_ymax = box_preds[..., 1] + box_preds[..., 3] / 2\n\n        box_true_xmin = box_true[..., 0]\n        box_true_ymin = box_true[..., 1]\n        box_true_xmax = box_true[..., 0] + box_true[..., 2]\n        box_true_ymax = box_true[..., 1] + box_true[..., 3]\n\n        x_min = torch.min(box_true_xmin, box_preds_xmin)\n        y_min = torch.min(box_true_ymin, box_preds_ymin)\n        x_max = torch.max(box_true_xmax, box_preds_xmax)\n        y_max = torch.max(box_true_ymax, box_preds_ymax)\n        rho = torch.sqrt((x_max - x_min) ** 2 + (y_max - y_min) ** 2)\n\n        x_true = box_true[..., 0] + (box_true[..., 2] / 2)\n        y_true = box_true[..., 1] + (box_true[..., 3] / 2)\n        w_true, h_true = box_true[..., 2], box_true[..., 3]\n        x_preds, y_preds, w_preds, h_preds = box_preds[..., 0], box_preds[..., 1], box_preds[..., 2], box_preds[..., 3]\n\n        norm = torch.sqrt((x_true - x_preds) ** 2 + (y_true - y_preds) ** 2)\n        v = (4.0 / (torch.pi ** 2)) * ((torch.atan(w_true / h_true) - torch.atan(w_preds / h_preds)) ** 2)\n        alpha = v / (1.0 - iou + 1e-8)\n        box_loss = (self.lbox / N_pos) * torch.sum((1.0 - iou + (norm ** 2) / (rho ** 2) + alpha * v))\n        return box_loss\n\n    def calculate_cls_loss(self, y_true, y_pred, N_pos):\n        y_true = F.one_hot(y_true, num_classes=NUM_CLASSES).float()\n        cls_loss = F.binary_cross_entropy(y_pred, y_true, reduction='sum')\n        cls_loss = (self.lcls / N_pos) * cls_loss\n        return cls_loss\n\n    def calculate_dfl_loss(self, box_true, box_preds, conf_preds, N_pos):\n        iou = calculate_iou(box_preds, box_true)\n\n        mask_right = (box_preds[..., 0:1] > box_preds[..., 0:1].unsqueeze(1))\n        mask_left = (box_preds[..., 0:1] < box_preds[..., 0:1].unsqueeze(1))\n\n        iou_diff_matrix = torch.abs(iou.unsqueeze(1) - iou.unsqueeze(0))\n        mask_self = torch.eye(iou.shape[0], dtype=torch.bool).to(device)\n        iou_diff_matrix = iou_diff_matrix.masked_fill(mask_self, float(\"inf\"))\n\n        iou_diff_right = iou_diff_matrix.masked_fill(~mask_right, float(\"inf\"))\n        nearest_iou_right_indices = torch.argmin(iou_diff_right, dim=1)\n        iou_p = iou[nearest_iou_right_indices]\n\n        iou_diff_left = iou_diff_matrix.masked_fill(~mask_left, float(\"inf\"))\n        nearest_iou_left_indices = torch.argmin(iou_diff_left, dim=1)\n        iou_n = iou[nearest_iou_left_indices]\n\n        del iou_diff_matrix\n\n        best_class_indices = torch.argmax(conf_preds, dim=-1)\n        conf_preds = torch.gather(conf_preds, dim=-1, index=best_class_indices.unsqueeze(-1)).squeeze(-1)\n        conf_preds_p = conf_preds[nearest_iou_right_indices]\n\n        dfl_loss = (-(iou_p - iou) * torch.log(conf_preds)) + ((iou - iou_n) * torch.log(conf_preds_p))\n        dfl_loss = (self.ldfl) * torch.sum(dfl_loss)\n\n        return dfl_loss\n\n    def forward(self, x):\n        img_feature = self.backbone(x)\n        box_preds, y_preds, conf_preds = self.head(img_feature)\n        return box_preds, y_preds, conf_preds\n\n    def compute_metrics(self, box_preds, y_preds, conf_preds, box_true, y_true):\n        box_preds, y_preds, conf_preds = self.select_preds(box_preds, box_true, y_preds, conf_preds)\n\n        mask = torch.any(box_true != torch.tensor([0, 0, 0, 0]).to(device), dim=-1)\n        box_true = box_true[mask]\n        y_true = y_true[mask]\n        box_preds = box_preds[mask]\n        y_preds = y_preds[mask]\n        conf_preds = conf_preds[mask]\n        N_pos = torch.sum(mask).float() / (3.0)\n\n        box_loss = self.calculate_box_loss(box_true, box_preds, N_pos)\n        cls_loss = self.calculate_cls_loss(y_true, y_preds, N_pos)\n        dfl_loss = self.calculate_dfl_loss(box_true, box_preds, conf_preds, N_pos)\n\n        return box_loss, cls_loss, dfl_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:18.952636Z","iopub.execute_input":"2024-11-19T13:46:18.952890Z","iopub.status.idle":"2024-11-19T13:46:18.970928Z","shell.execute_reply.started":"2024-11-19T13:46:18.952866Z","shell.execute_reply":"2024-11-19T13:46:18.970064Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"# Model\ntorch.cuda.empty_cache()\nbackbone = Backbone(image_size=IMAGE_SIZE)\nhead = MultiScaleHead(num_classes=NUM_CLASSES, num_bbox_pred_incell=BOX_PRED_INCELL)\nmodel = ObjectDetectionModel(backbone, head)\nmodel.to(device)\n\n# Early stopping configuration\npatience = 10\nbest_val_loss = float(\"inf\")\nearly_stop_count = 0\n\n# Learning rate schedule\nclass LRSchedule:\n    def __init__(self, post_warmup_learning_rate, warmup_steps):\n        self.post_warmup_learning_rate = post_warmup_learning_rate\n        self.warmup_steps = warmup_steps\n\n    def get_lr(self, step):\n        if step < self.warmup_steps:\n            return self.post_warmup_learning_rate * (step / self.warmup_steps)\n        else:\n            return self.post_warmup_learning_rate\n\nnum_train_steps = 2240 * EPOCHS\nnum_warmup_steps = num_train_steps // 15\nlr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n\n# Optimizer and scheduler\noptimizer = optim.Adam(model.parameters(), lr=lr_schedule.get_lr(0))\nscheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_schedule.get_lr(step))\n\n# Training and validation functions\ndef train_one_epoch(model, loader, optimizer, scheduler, device):\n    model.train()\n    total_box_loss, total_cls_loss, total_dfl_loss = 0, 0, 0 \n    for batch in tqdm(loader, desc=\"Training\"):\n        images, box_targets, class_targets = batch\n        images, box_targets, class_targets = images.to(device), box_targets.to(device), class_targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        # Calculate loss\n        box_loss, cls_loss, dfl_loss = model.compute_metrics(*outputs, box_targets, class_targets)\n        loss = box_loss + cls_loss + dfl_loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_box_loss += box_loss.item()\n        total_cls_loss += cls_loss.item()\n        total_dfl_loss += dfl_loss.item()\n        \n    avg_box_loss = total_box_loss / len(loader)\n    avg_cls_loss = total_cls_loss / len(loader)\n    avg_dfl_loss = total_dfl_loss / len(loader)\n    \n    return avg_box_loss, avg_cls_loss, avg_dfl_loss\n\ndef validate(model, loader, device):\n    model.eval()\n    total_box_loss, total_cls_loss, total_dfl_loss = 0, 0, 0 \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validation\"):\n            images, box_targets, class_targets = batch\n            images, box_targets, class_targets = images.to(device), box_targets.to(device), class_targets.to(device)\n            outputs = model(images)\n            # Calculate loss\n            box_loss, cls_loss, dfl_loss = model.compute_metrics(*outputs, box_targets, class_targets)\n            total_box_loss += box_loss.item()\n            total_cls_loss += cls_loss.item()\n            total_dfl_loss += dfl_loss.item()\n            \n    avg_box_loss = total_box_loss / len(loader)\n    avg_cls_loss = total_cls_loss / len(loader)\n    avg_dfl_loss = total_dfl_loss / len(loader)\n    \n    return avg_box_loss, avg_cls_loss, avg_dfl_loss\n\n# Training loop\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n\n    # Training step\n    loss_box, loss_cls, loss_dfl = train_one_epoch(model, train_loader, optimizer, scheduler, device)\n    print(f\"box_loss: {loss_box:.4f}  cls_loss: {loss_cls:.4f}  dfl_loss: {loss_dfl:.4f}\")\n\n    # Validation step\n    val_loss_box, val_loss_cls, val_loss_dfl = validate(model, val_loader, device)\n    print(f\"val_box_loss: {val_loss_box:.4f}  val_cls_loss: {val_loss_cls:.4f}  val_loss_dfl: {val_loss_dfl:.4f}\")\n\n    # Early stopping\n    if val_loss_box + val_loss_cls + val_loss_dfl < best_val_loss:\n        best_val_loss = val_loss_box + val_loss_cls + val_loss_dfl\n        early_stop_count = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        early_stop_count += 1\n        if early_stop_count >= patience:\n            print(\"Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T13:46:18.973179Z","iopub.execute_input":"2024-11-19T13:46:18.973473Z","iopub.status.idle":"2024-11-19T14:07:15.589166Z","shell.execute_reply.started":"2024-11-19T13:46:18.973448Z","shell.execute_reply":"2024-11-19T14:07:15.588265Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 174MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 140/140 [17:05<00:00,  7.32s/it]\n","output_type":"stream"},{"name":"stdout","text":"box_loss: 44.0470  cls_loss: 27.1162  dfl_loss: 12.7335\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 47/47 [03:50<00:00,  4.91s/it]","output_type":"stream"},{"name":"stdout","text":"val_box_loss: 44.0823  val_cls_loss: 27.1535  val_loss_dfl: 42.8517\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# next step: Non Maximum Suppression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T14:07:15.590576Z","iopub.execute_input":"2024-11-19T14:07:15.590929Z","iopub.status.idle":"2024-11-19T14:07:15.595282Z","shell.execute_reply.started":"2024-11-19T14:07:15.590891Z","shell.execute_reply":"2024-11-19T14:07:15.594445Z"}},"outputs":[],"execution_count":11}]}